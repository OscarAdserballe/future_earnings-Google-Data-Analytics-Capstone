{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Retrieval of Data\n",
    "My hypothesis requires me to collect data on the different choice of educations as well as their different entrance requirements. Most of this information is already contained in the dataset found on \"\"\", but it's missing the education's admission quotient. Each education (entry) is listed with a url to the website where this piece of information can be found through webscraping. Separate datasets for admission quotients do exist, but with no consistency in how the choice of educations are named, and therefore impossible to do a straight-forward .merge/join or VLOOKUP - webscraping rather than carefully aligning the format and becomes the simpler option (and probably less prone to errors).\n",
    "\n",
    "The data on future earnings is from surveys collected intermittently. Most survey data, hwever, is based on incredibly small sample sizes (most of which n < 20), not to mention a great share of them don't have any data on it at all. So, this limited dataset of ~300 choices of education is further narrowed down based on the availability of the data, and all numbers, even assuming that it's from a representative sample, is associated with low levels of confidence. Any conclusions drawn on this data have to be done so carefully and will inherently be limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modules used\n",
    "* requests for retrieving html pages associated with each url\n",
    "* bs4 (BeautifulSoup) for searching through the html page and getting the admission quotient slightly easier\n",
    "* pandas for retrieving data from excel file into a manageable dataframe allowing for easier manipulation of the data\n",
    "* sleep() from time module to avoid sending html requests in too rapid a succession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About the data\n",
    "As aforementioned, most of the data is contained in the dataset \"ufm_samlet_23jun2022\" except for the admission quotients. While it's necessary to look at the bachelor educations for the admission quotient, only the \"kandidatuddannelse\" of the same name has information regarding salary expectations. Proceeding to the \"kandidatuddannelse\" (basically a Master's) is usually guaranteed following admission in the bachelor's of the same name. The implications of this are, that initially, two dataframes are created based on the spreadsheet: one for the bachelors from which the admission quotients are retrieved; another for the \"Kandidatuddannelse\" which will contain the salary and job prospects. Only at the end will they be joined where the choice of education is the same - that is, we assume the choice of \"kandidatuddannelse\" will directly follow from the choice of bachelor's of same name. (Note, this mapping is not always possible, and therefore some entries will be lost in the joining of the dataframes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"ufm_samlet_23jun2022.xlsx\")\n",
    "\n",
    "# the columns used from the spreadsheet\n",
    "df = df[[\"Titel\", \"url\", \"displaydocclass\", \"hovedinsttx\",\\\n",
    "         \"ledighed_nyudd\", \"maanedloen_nyudd\", \"ledighed_10aar\", \"maanedloen_10aar\"]]\n",
    "\n",
    "# df for \"kandidatuddannelse\" and job and salary prospects. Also, institution and title of education choice for identifier\n",
    "# Clearing entries where \"Uddannelsen på landsplan\" as they're just an agglomeration of the other entries when you can take\n",
    "# the same education at multiple institutions\n",
    "df_kandidat = df.loc[(df[\"displaydocclass\"] == \"Kandidatuddannelse\") & (df[\"hovedinsttx\"] != \"Uddannelsen på landsplan\")]\n",
    "df_kandidat = df_kandidat[[\"Titel\", \"hovedinsttx\", \"ledighed_nyudd\", \"maanedloen_nyudd\", \"ledighed_10aar\", \"maanedloen_10aar\"]]\n",
    "\n",
    "# df for bachelor's. Only getting url for admission quotients and title and institution for unique identifier\n",
    "df_bachelor = df.loc[(df[\"displaydocclass\"] == \"Bacheloruddannelse\") & (df[\"hovedinsttx\"] != \"Uddannelsen på landsplan\")]\n",
    "df_bachelor = df_bachelor[[\"Titel\", \"url\", \"hovedinsttx\"]]\n",
    "\n",
    "df_kandidat.reset_index(inplace=True)\n",
    "df_bachelor.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Webscraping\n",
    "\n",
    "The goal here is to get an array of the admission quotients, such that it can easily be added as a column later. This requires 1:1 overlap with the entries, and through trial and error I found out storing the titles was useful to ensure no duplicates in the list. \n",
    "\n",
    "The admission quotient is always stored in the same format inside the div with the class \"views-row views-row-6\", so too creative an endeavour, this webscraping was not.\n",
    "\n",
    "A few exceptions do emerge and they're simply stored for manual inspection. Upon looking at the 4 problematic entries, it becomes clear that it happens, because the dataset is not fully up-to-date but is a year old: these problematic entries are remnants of a lost time, when one could portugese-and-brasilian studies. They're just deleted later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem with https://www.ug.dk/uddannelser/bachelorogkandidatuddannelser/bacheloruddannelser/humanistiskebacheloruddannelser/sprog/portugisiske-og-brasilianske-studier\n",
      "Whose index is, 141\n",
      "index                                                       1518\n",
      "Titel                       Portugisiske og brasilianske studier\n",
      "url            www.ug.dk/uddannelser/bachelorogkandidatuddann...\n",
      "hovedinsttx                               Københavns Universitet\n",
      "Name: 141, dtype: object\n",
      "\n",
      "\n",
      "Problem with https://www.ug.dk/uddannelser/bachelorogkandidatuddannelser/bacheloruddannelser/naturvidenskabeligebacheloruddannelser/biologigeografigeologimv/biokemi-og-molekylaer-biologi\n",
      "Whose index is, 161\n",
      "index                                                       1549\n",
      "Titel                               Biokemi og molekylær biologi\n",
      "url            www.ug.dk/uddannelser/bachelorogkandidatuddann...\n",
      "hovedinsttx                                 Syddansk Universitet\n",
      "Name: 161, dtype: object\n",
      "\n",
      "\n",
      "Problem with https://www.ug.dk/uddannelser/bachelorogkandidatuddannelser/bacheloruddannelser/tekniskvidenskabeligebacheloruddannelser/civilingenioeruddannelser/produkt-og-designpsykologi\n",
      "Whose index is, 222\n",
      "index                                                       1863\n",
      "Titel                                Produkt- og designpsykologi\n",
      "url            www.ug.dk/uddannelser/bachelorogkandidatuddann...\n",
      "hovedinsttx                                  Aalborg Universitet\n",
      "Name: 222, dtype: object\n",
      "\n",
      "\n",
      "Problem with https://www.ug.dk/uddannelser/bachelorogkandidatuddannelser/bacheloruddannelser/tekniskvidenskabeligebacheloruddannelser/civilingenioeruddannelser/elektronik\n",
      "Whose index is, 327\n",
      "index                                                       2197\n",
      "Titel                                                 Elektronik\n",
      "url            www.ug.dk/uddannelser/bachelorogkandidatuddann...\n",
      "hovedinsttx                                 Syddansk Universitet\n",
      "Name: 327, dtype: object\n",
      "\n",
      "\n",
      "Done executing\n"
     ]
    }
   ],
   "source": [
    "# Getting quotients from bachelor admission\n",
    "quotients = []\n",
    "# storing webscraped education choices to ensure that the same is not cited multiple times\n",
    "titles_done = []\n",
    "\n",
    "# iterating through each url (entry)\n",
    "# iterating through df painfully bad, but.... it was easier\n",
    "for index, row in df_bachelor.iterrows():\n",
    "    quotient_temp = []\n",
    "\n",
    "    fixed_url = \"https://\" + row[\"url\"]\n",
    "    r = requests.get(fixed_url)\n",
    "    try:\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        div = soup.find(\"div\", class_=\"views-row views-row-6\")\n",
    "        table = div.find(\"table\")\n",
    "\n",
    "        tbody = table.find(\"tbody\")\n",
    "        trs = tbody.find_all(\"tr\")\n",
    "        \n",
    "        \n",
    "        for tr in trs:\n",
    "            if tr.find(\"b\"):\n",
    "                continue\n",
    "            else:\n",
    "                tds = tr.find_all(\"td\")              \n",
    "                sub_counter = 0\n",
    "                for td in tds:\n",
    "                    # pattern of every third,\n",
    "                    # 0 locatoin\n",
    "                    # 1 average required to get in immediately\n",
    "                    # 2 average for \"standby quotient\"\n",
    "                    if sub_counter % 3 == 1:\n",
    "                        quotient_temp.append(td.text)\n",
    "\n",
    "                    sub_counter += 1\n",
    "                    \n",
    "        if row[\"Titel\"] not in titles_done:\n",
    "            quotients.append(quotient_temp)\n",
    "            titles_done.append(row[\"Titel\"])\n",
    "            \n",
    "        sleep(0.2) # -> will take about 300 * 0.2 seconds = 1 minute\n",
    "        \n",
    "        \n",
    "    # # exception if choice of study no longer exists. It will spit out the index number\n",
    "    # and the data associated with it, such that manual inspection is possible\n",
    "    except AttributeError: \n",
    "        print(\"Problem with\", fixed_url)\n",
    "        print(\"Whose index is,\", index)\n",
    "        print(row)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "print(\"Done executing\")\n",
    "        # problems with indexes\n",
    "        # 141, 161, 222, 327"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing old data\n",
    "After running, issues arise with indexes 141, 161, 222, 327. They are dropped from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping problematic indexes -> choice of study no longer exists [verified manually]\n",
    "problematic_indexes = [141, 161, 222, 327]\n",
    "for i in problematic_indexes:\n",
    "    df_bachelor.drop(i, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Admission Quotients\n",
    "There's still one error with the array of admission quotients: it has a length of 325, whereas the dataframe has a length of 324. For some reason, \"Kun kvote 2\" snuck in. Beyond that, there's also the question of how to deal with \"AO\". Most universities have enough places for everyone who tried to apply got in, so there was effectively no admission quotient. That's translated to become a 0 in the dataset, but I'm still not sure there weren't better solutions for that, as it looks weird on data visualisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotient_column = [j for i in quotients for j in i if j != \"Kun kvote 2\"]\n",
    "quotient_column = [float(x.replace(\",\", \".\")) if x != \"AO\" else 0 for x in quotient_column]\n",
    "df_bachelor = df_bachelor.assign(Adgangskvotienter=quotient_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging the Two Dataframes\n",
    "To address the lack of a common unique identifier that could be used to join the two dataframes without error, I opted to create a new one. This one combines the title of the choice of education and the institution in a common format across the dataframes.\n",
    "\n",
    "After that, the two databases can be joined on that column. Some columns are dropped as they are superfluous. The same goes for duplicate entries that have persisted from the original dataset used. Finally, the dataframe is converted into a CSV file so it's ready for R and Tableau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kandidat[\"Uddannelse\"] = df_kandidat[\"Titel\"] + \" - \" + df_kandidat[\"hovedinsttx\"]\n",
    "df_bachelor[\"Uddannelse\"] = df_bachelor[\"Titel\"] + \" - \" + df_bachelor[\"hovedinsttx\"]\n",
    "df2 = df_bachelor.merge(df_kandidat, left_on=\"Uddannelse\", right_on=\"Uddannelse\")\n",
    "df2.drop(columns=[\"Titel_x\", \"hovedinsttx_x\", \"index_x\", \"index_y\", \"Titel_y\", \"hovedinsttx_y\"], inplace=True)\n",
    "df2.reset_index(inplace=True)\n",
    "df2 = df2.drop_duplicates(subset=[\"Uddannelse\", \"url\"])\n",
    "df2.to_csv(\"education_earnings_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
